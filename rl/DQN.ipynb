{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtsM5fhPgEuc"
      },
      "source": [
        "# Practical Session DQN\n",
        "\n",
        "In this practical session, you will implement the famous DQN algorithm and test it in various environments.\n",
        "\n",
        "Run the following two cells if you are using Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMvOBHKTrkZv",
        "outputId": "c6ffef24-a8f4-481c-f6df-a8f983070b4c"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install -q swig\n",
        "!pip install -q gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3DsN79PDxHj"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZJD6LPEr27z"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "from IPython.display import Image\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def display_trajectory(frames, fps=25):\n",
        "  imageio.mimwrite('./lunar_lander.gif',\n",
        "                [np.array(frame) for i, frame in enumerate(frames)],\n",
        "                fps=fps)\n",
        "  return(Image(open('lunar_lander.gif','rb').read()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnWbiTnogjch"
      },
      "source": [
        "# Lunar Lander\n",
        "[Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) is a reinforcement learning toy environment provided with the Gym Library.  \n",
        "In this environment, you control the reactors of a Lander that tries to land inside a landing area as smoothly as possible.\n",
        "The official environment description is  \n",
        "*Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.*\n",
        "\n",
        "The environment will provide observations to the agent in the form of an 8-dimensional vector composed of:\n",
        "* Position X\n",
        "* Position Y\n",
        "* Velocity X\n",
        "* Velocity Y\n",
        "* Angle\n",
        "* Angular Velocity\n",
        "* Is left leg touching the ground: 0 OR 1\n",
        "* Is right leg touching the ground: 0 OR 1  \n",
        "\n",
        "The agent can choose between four discrete actions:\n",
        "* 0 = Do Nothing\n",
        "* 1 = Fire Left Engine\n",
        "* 2 = Fire Main Engine\n",
        "* 3 = Fire Right Engine  \n",
        "\n",
        "At every transition, the agent will be rewarded by the environment according to the following reward function:\n",
        "\n",
        "* [100, 140] points for Moving to the landing pad and zero speed\n",
        "* Negative reward for moving away from the landing pad\n",
        "* If the lander crashes or comes to rest, it gets -100 or +100\n",
        "* Each leg with ground contact gets +10\n",
        "* Firing the main engine is -0.3 per frame\n",
        "* Firing the side engine is -0.03 per frame\n",
        "* 200 points for solving the environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Yaa8cSiXy-w"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import sys\n",
        "from time import time\n",
        "from collections import deque, defaultdict, namedtuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "from IPython.display import Image\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx7AYLSnX7PE"
      },
      "outputs": [],
      "source": [
        "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfLhHuhJYA3W",
        "outputId": "0e350e8d-a9a3-4727-a78e-c3d06acbc628"
      },
      "outputs": [],
      "source": [
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQpj2vkyivwL"
      },
      "source": [
        "Here is a little code snippet to visualise a random agent playing Lunar Lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "bJbp4SnIDuEj",
        "outputId": "94a6e0da-1a63-421e-8041-3bc0f46d51ad"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "obs = env.reset()\n",
        "img = env.render()\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = env.action_space.sample()\n",
        "    obs, _, done, _ ,_ = env.step(action)\n",
        "    img = env.render()\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "\n",
        "display_trajectory(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWcGoe9ZjzF4"
      },
      "source": [
        "We will now implement and train a DQN agent.\n",
        "Fill the following code to create a neural network class that our agent will use.\n",
        "The network should be small enough not to slow down the training process.  \n",
        "You can use a two hidden layers neural network with 512 and 256 hidden neurons using a ReLU activation function and a final layer with as many neurons as the environment action space using a linear activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lnVAsTZgOnF"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = ...\n",
        "        self.fc2 = ...\n",
        "        self.fc3 = ...\n",
        "\n",
        "    def forward(self, x):\n",
        "        ...\n",
        "\n",
        "        return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pA_sdNSqj224"
      },
      "source": [
        "The following class implements a replay buffer that will act as our agent memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQS7b95VgPlN"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        \"\"\"\n",
        "        Replay memory allow agent to record experiences and learn from them\n",
        "\n",
        "        Parametes\n",
        "        ---------\n",
        "        buffer_size (int): maximum size of internal memory\n",
        "        batch_size (int): sample size from experience\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add experience\"\"\"\n",
        "        experience = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"\n",
        "        Sample randomly and return (state, action, reward, next_state, done) tuple as torch tensors\n",
        "        \"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        states = torch.from_numpy(np.vstack([experience.state for experience in experiences if experience is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([experience.action for experience in experiences if experience is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([experience.reward for experience in experiences if experience is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([experience.next_state for experience in experiences if experience is not None])).float().to(device)\n",
        "        # Convert done from boolean to int\n",
        "        dones = torch.from_numpy(np.vstack([experience.done for experience in experiences if experience is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-YTCBEQDuEm"
      },
      "source": [
        "# DQN\n",
        "The fllowing class implements a DQN agent.  \n",
        "Its training procedure is located in the `train` method.  \n",
        "Fill the missing parts of the `act` and `learn` methods according to the DQN algorithm:\n",
        "![](https://github.com/DavidBert/N7-techno-IA/blob/master/code/reinforcement_learning/images/dqn.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwxcpmGZgTEs"
      },
      "outputs": [],
      "source": [
        "LR = 1e-3               # Q Network learning rate\n",
        "EPS_DECAY = 0.999    # Epsilon decay rate\n",
        "EPS_MIN = 0.01  #min Epsilon\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def print_running_mean(training_rewards):\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(15,3))\n",
        "    plt.plot(pd.Series(training_rewards).rolling(10).mean())\n",
        "    plt.title(\"Rewards running mean on last 100 episodes\")\n",
        "    plt.show()\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, env, gamma=0.99, memory_size=100000, batch_size=256, update_rate=4):\n",
        "        self.env =  env\n",
        "        self.gamma = gamma\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        self.update_rate = 4\n",
        "        self.batch_size = batch_size\n",
        "        self.q_network = QNetwork(self.state_size, self.action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001, weight_decay=1e-6)\n",
        "        # Initiliase memory\n",
        "        self.memory = ReplayBuffer(memory_size, batch_size)\n",
        "        self.timestep = 0\n",
        "        self.epsilon = 1\n",
        "\n",
        "\n",
        "    def train(self, max_steps):\n",
        "        self.timestep = 0\n",
        "        self.epsilon = 1\n",
        "\n",
        "        state, _ = self.env.reset()\n",
        "        current_reward = 0\n",
        "        episode_rewards = []\n",
        "        for _ in range(max_steps):\n",
        "            action = self.act(state, train=True)\n",
        "            next_state, reward, done, _, info = self.env.step(action)\n",
        "            current_reward += reward\n",
        "            self.memory.add(state, action, reward, next_state, done)\n",
        "            self.timestep += 1\n",
        "            if (self.timestep % self.update_rate == 0) & (len(self.memory) > self.batch_size):\n",
        "                self.learn()\n",
        "\n",
        "            self.epsilon = max(self.epsilon * EPS_DECAY, EPS_MIN)\n",
        "            if done:\n",
        "                state, _ = self.env.reset()\n",
        "                episode_rewards.append(current_reward)\n",
        "                current_reward = 0\n",
        "\n",
        "            else:\n",
        "                state = next_state\n",
        "            if self.timestep % 1000 == 0:\n",
        "                print_running_mean(episode_rewards)\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        experiences_batch = ... #get a minibatch from the replay buffer\n",
        "        states, actions, rewards, next_states, dones = experiences_batch\n",
        "        # Get the action with max Q value\n",
        "        action_values = self.q_network(next_states).detach() #Do you know why is this detach important?\n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # If done just use reward, else update Q_target with discounted action values\n",
        "        Q_target = rewards + (self.gamma * max_action_values * (1 - dones))\n",
        "        Q_expected = self.q_network(states).gather(1, actions)\n",
        "\n",
        "        loss = ...\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def act(self, state, train=False):\n",
        "\n",
        "        if (random.uniform(0, 1) < self.epsilon) & train:\n",
        "            return ...\n",
        "        else:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                action_values = ...#the predicted Qvalues\n",
        "            action = np.argmax(action_values.cpu().data.numpy())\n",
        "            return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4TBIZ8jlcIV"
      },
      "source": [
        "Instanciate a DQN agent and train it on lunar lander for at least 100000 steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "-lMIifWGDuEp",
        "outputId": "bb93c7f5-dcb5-4948-bbcf-291394421950"
      },
      "outputs": [],
      "source": [
        "dqn_agent = ...\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fna62-OWoLsg"
      },
      "source": [
        "Complete the following code to visualize your agent playing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "ZH2su8-2DuEp",
        "outputId": "0fd70626-9db2-4531-e1ac-20967f61273d"
      },
      "outputs": [],
      "source": [
        "obs, _ = env.reset()\n",
        "img = env.render()\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = ...\n",
        "    obs, _, done,_ ,_ = env.step(action)\n",
        "    img = env.render()\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "display_trajectory(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVUOs7jll5Xs"
      },
      "source": [
        "An important impovement of DQN is the use of a target network.  \n",
        "Modify your DQN agent to use a secondary target network.  \n",
        "![](https://github.com/DavidBert/N7-techno-IA/raw/master/code/reinforcement_learning/images/dqn_target.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3b-UoJCDuEr"
      },
      "outputs": [],
      "source": [
        "class DQNTargetAgent(DQNAgent):\n",
        "    def __init__(self, env, gamma=0.99, memory_size=100000, batch_size=256, update_rate=4, target_update_rate=1000):\n",
        "        super().__init__(env, gamma, memory_size, batch_size, update_rate)\n",
        "        self.target_update_rate = target_update_rate\n",
        "        self.target_network = ....to(device)\n",
        "        self.target_network.load_state_dict(...) #initialise the target network weights with the policy network's weights\n",
        "\n",
        "    def learn(self):\n",
        "        experiences_batch = self.memory.sample()\n",
        "        states, actions, rewards, next_states, dones = experiences_batch\n",
        "        # Get the action with max Q value\n",
        "        action_values = ... #Use the target network\n",
        "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
        "\n",
        "        # If done just use reward, else update Q_target with discounted action values\n",
        "        Q_target = rewards + (self.gamma * max_action_values * (1 - dones))\n",
        "        Q_expected = ...\n",
        "\n",
        "        loss = F.mse_loss(Q_expected, Q_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        if self.timestep % self.target_update_rate == 0: #update the target network weights with the policy network's weights\n",
        "            ...  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train your agent on 100000 steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "ieI4TJEGghuF",
        "outputId": "f61a8faf-773f-4d5e-dd10-231179be604f"
      },
      "outputs": [],
      "source": [
        "dqn_target_agent = ...\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IHzWpU7of98"
      },
      "source": [
        "Visualize your agent playing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYzJQmz0DuEr"
      },
      "outputs": [],
      "source": [
        "obs, _ = env.reset()\n",
        "img = env.render()\n",
        "\n",
        "while True:\n",
        "    ...\n",
        "\n",
        "env.close()\n",
        "display_trajectory(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBK075MfokmX"
      },
      "source": [
        "The following code compares all the agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bC84Y4FZDuEs"
      },
      "outputs": [],
      "source": [
        "max_episodes = 100\n",
        "\n",
        "df = pd.DataFrame(columns=['rewards', 'agent'])\n",
        "random_fcn = lambda x: np.random.choice(env.action_space.n)\n",
        "dqn_fcn = lambda x: dqn_agent.act(x)\n",
        "target_dqn_fcn = lambda x: dqn_target_agent.act(x)\n",
        "\n",
        "for agent, fcn in zip(['random', 'dqn', 'target_dqn'], [random_fcn, dqn_fcn, target_dqn_fcn]):\n",
        "    scores = []\n",
        "    for i in tqdm(range(max_episodes)):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        while True:\n",
        "            action = fcn(state)\n",
        "            state, reward, done,_ , info = env.step(action)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores.append(score)\n",
        "    for score in scores:\n",
        "        df = df.append({'rewards': score, 'agent': agent}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVoMC79Xi81N"
      },
      "source": [
        "# Cartpole\n",
        "\n",
        "CartPole is another [toy control environment avalible in OpeAI Gym](https://gym.openai.com/envs/CartPole-v0/).\n",
        "*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.*\n",
        "\n",
        "It is also known as the inverted pendulum problem.  \n",
        "In this environment the objective is to balance a pole as long as possible. It is assumed that at the tip of the pole, there is an object which makes it unstable and very likely to fall over. The agent controls the cart and must move left and right so that the pole can stand as long as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GePTgwYFDuEs"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "tVI6Rhv1DuEt",
        "outputId": "786bb70f-3c20-4cd8-8ae1-23ed51988886"
      },
      "outputs": [],
      "source": [
        "images = []\n",
        "obs, _ = env.reset()\n",
        "img = env.render()\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = env.action_space.sample()\n",
        "    obs, _, done, _,_ = env.step(action)\n",
        "    img = env.render()\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "display_trajectory(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDMbTPmrp2LV"
      },
      "source": [
        "Train a dqn agent on 10000 steps to solve cartpole."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "shLtAxhki8Da",
        "outputId": "afd461fe-b191-4fdd-d698-4b06379ef8c6"
      },
      "outputs": [],
      "source": [
        "dqn_agent = ...\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWQSCBQpDuEu"
      },
      "outputs": [],
      "source": [
        "obs, _ = env.reset()\n",
        "img = env.render()\n",
        "\n",
        "while True:\n",
        "    ...\n",
        "\n",
        "env.close()\n",
        "display_trajectory(images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00SrU1GwqESW"
      },
      "source": [
        "Train a dqn agent to solve cartpole using target network and compare its performances with the simple DQN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "496w98bDDuEu"
      },
      "outputs": [],
      "source": [
        "dqn_target_agent = ...#you may update the target network every 500 steps\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4Z65WQtDuEv"
      },
      "outputs": [],
      "source": [
        "obs, _ = env.reset()\n",
        "img = env.render()\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action = dqn_target_agent.act(obs)\n",
        "    obs, _, done, _,_ = env.step(action)\n",
        "    img = env.render()\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "display_trajectory(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgzubhZHDuEv"
      },
      "outputs": [],
      "source": [
        "max_episodes = 100\n",
        "\n",
        "df = pd.DataFrame(columns=['rewards', 'agent'])\n",
        "random_fcn = lambda x: np.random.choice(env.action_space.n)\n",
        "dqn_fcn = lambda x: dqn_agent.act(x)\n",
        "target_dqn_fcn = lambda x: dqn_target_agent.act(x)\n",
        "\n",
        "for agent, fcn in zip(['random', 'dqn', 'target_dqn'], [random_fcn, dqn_fcn, target_dqn_fcn]):\n",
        "    scores = []\n",
        "    for i in tqdm(range(max_episodes)):\n",
        "        state, _ = env.reset()\n",
        "        score = 0\n",
        "        while True:\n",
        "            action = fcn(state)\n",
        "            state, reward, done, _, info = env.step(action)\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores.append(score)\n",
        "    for score in scores:\n",
        "        df = df.append({'rewards': score, 'agent': agent}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7wlteZoGdij"
      },
      "source": [
        "# Stable-baselines3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUQSNcugqO0k"
      },
      "source": [
        "Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It contains efficient implementations of the most famous RL algorithm and is certainly one of the most simple way to prototype an RL agent.  \n",
        "A full description of the librairy is available [here](https://stable-baselines3.readthedocs.io/en/master/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDrQonYsIJc-"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_1m6EYrBG5"
      },
      "source": [
        "Help yourself with the official documentation and train a dqn agent using Stable baselines3 on Lunar lander."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mvUTfx3Gg5s"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "\n",
        "# Create environment\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "# Instantiate the agent\n",
        "model = DQN('MlpPolicy', env, verbose=1)\n",
        "# Train the agent\n",
        "model.learn(total_timesteps=int(2e5))\n",
        "# Save the agent\n",
        "model.save(\"dqn_lunar\")\n",
        "del model  # delete trained model to demonstrate loading\n",
        "\n",
        "# Load the trained agent\n",
        "model = DQN.load(\"dqn_lunar\", env=env)\n",
        "\n",
        "# Evaluate the agent\n",
        "# NOTE: If you use wrappers with your environment that modify rewards,\n",
        "#       this will be reflected here. To evaluate with original rewards,\n",
        "#       wrap environment in a \"Monitor\" wrapper before other wrappers.\n",
        "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
        "\n",
        "# Enjoy trained agent\n",
        "obs = env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, rewards, dones, info = env.step(action)\n",
        "    env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6D0qRYrMF3i"
      },
      "outputs": [],
      "source": [
        "\n",
        "scores = []\n",
        "for i in tqdm(range(max_episodes)):\n",
        "    state = env.reset()\n",
        "    score = 0\n",
        "    while True:\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scores.append(score)\n",
        "for score in scores:\n",
        "    df = df.append({'rewards': score, 'agent': 'stable_baseline'}, ignore_index=True)\n",
        "\n",
        "sns.boxplot(x=\"agent\", y=\"rewards\",data=df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vwCiho6MmzR"
      },
      "outputs": [],
      "source": [
        "obs = env.reset()\n",
        "img = env.render(mode='rgb_array')\n",
        "\n",
        "while True:\n",
        "    # At each step, append an image to list\n",
        "    images.append(img)\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, _, done ,_ = env.step(action)\n",
        "    img = env.render(mode='rgb_array')\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "env.close()\n",
        "imageio.mimwrite('./stable_baselines.gif',\n",
        "                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "                fps=29)\n",
        "\n",
        "Image(open('stable_baselines.gif','rb').read())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "INSA_DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
