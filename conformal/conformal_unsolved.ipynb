{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt9mGio-2Mmx"
      },
      "source": [
        "# Part I: Conformal Regression\n",
        "\n",
        "We consider a simple regression problem on heteroskedastic data. We want to evaluate the uncertainty associated with the prediction using various conformal prediction methods. The main objective of this first part is to get a better grasp of how Conformal Prediction works, and to *visualize* the effect of the different algorithms on the coverage rate and the size of the prediction intervals. We will code most of the algorithms from scratch, and compare our results with those obtained with help of the PUNCC library for verification purposes.\n",
        "\n",
        "**Links**\n",
        "- [PUNCC Github](https://github.com/deel-ai/puncc)\n",
        "- [PUNCC Documentation](https://deel-ai.github.io/puncc/index.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnzN8HZq2Mmz"
      },
      "source": [
        "## 0. Setup\n",
        "\n",
        "**Exercise.** Install the puncc library using pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_V2OZPz2Mm0"
      },
      "outputs": [],
      "source": [
        "# TODO: install the PUNCC library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTOFV7ls2Mm1"
      },
      "source": [
        "We import some of the libraries that we will be using throughout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wODidhUv2Mm1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjsP1eXm2Mm2"
      },
      "source": [
        "## 1. Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d-qM8ND2Mm2"
      },
      "source": [
        "We consider a synthetic 1D heteroskedastic dataset, where the variance of the noise increases with the value of the input feature.\n",
        "We generate $N$ samples as follows:\n",
        "\n",
        "- Inputs $X$ are uniformly distributed on $[0, 20]$\n",
        "- Outputs are given by $Y = (1+\\epsilon)\\cdot X, $\n",
        "\n",
        "Such that $\\epsilon \\sim {\\cal N}(\\mu=0,\\sigma=1)$ is standard gaussian noise.\n",
        "\n",
        "**Exercise.**\n",
        "1. Complete the function `heteroskedastic_data` that takes the number of samples `n_samples` to be generated as an arguments, and outputs numpy arrays `X` and `y` of size `n_samples` according to the procedure described above.\n",
        "2. Use the `heteroskedastic_data` function to generate 4000 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faWFg9ev2Mm2"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic 1D heteroskedastic data\n",
        "\n",
        "def heteroskedastic_data(n_samples):\n",
        "    # TODO: complete\n",
        "\n",
        "\n",
        "# TODO: generate 4000 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Complete the function `plot_data below`, and use it to plot the synthetic data."
      ],
      "metadata": {
        "id": "YSQw03buBa5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(X, y):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # TODO: plot the data, what plot type would you choose?\n",
        "    plt.xlabel(r\"$X$\")\n",
        "    plt.ylabel(r\"$Y$\")\n",
        "    plt.xticks(np.arange(22, step=2))\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "# TODO: plot the data"
      ],
      "metadata": {
        "id": "-2er3CFeA_gD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Split the data into a training set and a test set by using the `train_test_split` function of the `scikit-learn` library. Split the data randomly and leaving out 25% of the data for the test set."
      ],
      "metadata": {
        "id": "O5ijwE1xBrMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Split the data"
      ],
      "metadata": {
        "id": "NevHayn9_FuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Split Conformal Regression\n",
        "\n",
        "In order to perform the *split conformal regression* algorithm, we need to split our training data into a *proper training set* (which we will store in the variables `X_fit` and `y_fit`) and a *calibration set*.\n",
        "\n",
        "**Exercise.** Further split the training data randomly by leaving out 50% of the training data for the calibration set."
      ],
      "metadata": {
        "id": "-zM53vaUHacq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9kwSokT2Mm4"
      },
      "outputs": [],
      "source": [
        "# TODO: Split the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Train a prediction model using the `LinearRegression` class in the `scikit-learn` library."
      ],
      "metadata": {
        "id": "P15suIFMK-IS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0erSYk42Mm3"
      },
      "outputs": [],
      "source": [
        "# TODO: Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Complete the `plot_model` function below in order to plot synthetic data along with the model predictions. Use the function `plot_model` to plot the test data along with the model predictions."
      ],
      "metadata": {
        "id": "-38rY-fcMAQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model(model, X, y):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # TODO: plot the data\n",
        "    # TODO: plot the model predictions\n",
        "    plt.xlabel(r\"$X$\")\n",
        "    plt.ylabel(r\"$Y$\")\n",
        "    plt.xticks(np.arange(22, step=2))\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "# TODO: visualize the data and model in the same plot"
      ],
      "metadata": {
        "id": "HaCk0pvdMH0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a trained model, we wish to *conformalize* it. The simplest algorithm to conformalize a model is the *split conformal* algorithm.\n",
        "\n",
        "In this section, we are using the split conformal algorithm for the regression task, but as we have seen in the lectures, there are many other algorithms conformalization algorithms that rely on splitting the data into fit and calibration setl. Therefore, we will coda a generc class called `SplitConformal` that we will be able to use later for other conformal prediction algorithms of the *split* type.\n",
        "\n",
        "In order to define our general `SplitConformal` class, we will rely on the following information:\n",
        "the algorithms using the `SplitConformal` class are different between each other only in two ways:\n",
        "- the way the *nonconformity scores* are computed,\n",
        "- the way the *prediction sets* are constructed.\n",
        "\n",
        "Therefore, we will build the `SplitConformal` class so that it can work with any choice of *nonconformity score function* a *prediction set contruction function*.\n",
        "\n",
        "\n",
        "**Exercise.** Code the `SplitConformal` class by implementing the following:\n",
        "1. Attributes:\n",
        "  - `score_fn`: the nonconformity score function to be used (a function that computes scores between ground truth values and model predictions).\n",
        "  - `predset_fn`: the prediction set construction function to be used (a function that builds prediction sets from model predictions and quantile values).\n",
        "  - `scores`: attribute to store the array of calibration nonconformity scores.\n",
        "  - `quantile`: attribute to store the value of the quantile of order $1-\\alpha$ (plus correction).\n",
        "\n",
        "2. Methods:\n",
        "  - `__init__`: takes as input the choice of nonconformity score function and prediction set construction function.\n",
        "  - `compute_scores`: takes as input a numpy array containing ground truth $y$-values and predicted $y$-values, computes the nonconformity scores, and stores them into the attribute `scores`.\n",
        "  - `compute_quantile`: takes as input the nominal error rate $\\alpha$ and computes the quantile of level $1-\\alpha$ (taking into account the usual finite-sample correction characteristic of conformal prediction algorithms) on the array of scores saved in the `scores` attribute. **Warning!** You may use the `quantile` function in the `numpy` library, however, make sure you choose the *right* method to compute the quantile values, otherwise the probabilistic guarantee of conformal prediction will no longer be true. (**This is an important point, try and understand the different methods and choose the correct one yourself before checking the solutions!!!**)\n",
        "  - `predict`: takes as input an array of model predictions and outputs prediction sets (in whatever format the `predset_fn` outputs prediction sets)."
      ],
      "metadata": {
        "id": "TFdTY1rZwvOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SplitConformal():\n",
        "    # TODO: Complete the class"
      ],
      "metadata": {
        "id": "K-jBZs8aw1yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Code the `abs_difference` and `additive_interval` functions to be used as the arguments of the `SplitConformal` class at initialization."
      ],
      "metadata": {
        "id": "hhzDBc3P1fqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: code both functions"
      ],
      "metadata": {
        "id": "DjvWD01e1X82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Use the `SplitConformal` class to conformalize the Linear Regression model with a nominal error rate of 0.1."
      ],
      "metadata": {
        "id": "0WkxO_jd1O_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: conformalize"
      ],
      "metadata": {
        "id": "GTOs_VZo2XLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Complete the function `plot_conformalized_data` below and use it to plot the test datset along with the model predictions and the prediction intervals."
      ],
      "metadata": {
        "id": "1An-2gDM3ILU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_conformalized_data(X, y, y_pred, y_lower, y_upper):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sort_indices = np.argsort(X.flatten())\n",
        "\n",
        "    # TODO: plot the data\n",
        "    # TODO: plot the model predictions\n",
        "    # TODO: plot the prediction interval\n",
        "    plt.xlabel(r\"$X$\")\n",
        "    plt.ylabel(r\"$Y$\")\n",
        "    plt.xticks(np.arange(22, step=2))\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "plot_conformalized_data(X_test, y_test, y_pred_test, y_lower, y_upper)"
      ],
      "metadata": {
        "id": "2foMQULL3T8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Write a function called `evaluate_conformal_regression` that takes as input an array of ground-truth $y$-values along with an array of the lower limits and an array of the upper limits of the corresponding prediction intervals. It then computes and outputs the following two metrics:\n",
        "1. `coverage`: The average number of intervals that contain the ground-truth values.\n",
        "2. `avg_length`: The average length of the intervals.\n",
        "\n",
        "**Questions.**\n",
        "1. What value should we expect for the `coverage` metric?\n",
        "2. What kind of values do we desire for the `avg_length` metric?\n"
      ],
      "metadata": {
        "id": "_o0G1uve_CNH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement the evaluate_conformal_regression function"
      ],
      "metadata": {
        "id": "AECk58vD-Jih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Evaluate the conformalized model with the help of the `evaluate_conformal_regression` function and display the results."
      ],
      "metadata": {
        "id": "SijSFsr0l81R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: evaluate the conformal prediction regressor"
      ],
      "metadata": {
        "id": "d0ou8AO8l3O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation seems to match the desired results. Note however (from the plot) how the interval length is constant, and does not match the heteroskedasticity of the data.\n",
        "\n",
        "Next, we perform a similar conformalization procedure using the PUNCC library, this will allow us to compare results, but also to learn how the PUNCC library works so that we can use it in the future instead of coding the CP algorithms from scratch.\n",
        "\n",
        "**Exercise.** Find the tutorial called *Introduction Tutorial* in the Readme page of the PUNCC github repository and dollow the steps presented in the *Conformal Regression* section in order to conformalize the linar regression model above using PUNCC:\n",
        "1. Train the model using PUNCC rather than fitting it directly.\n",
        "2. Compute the prediciton intervals.\n",
        "3. Compute the evaluation metrics.\n",
        "4. Use PUNCC's visualization tools to visualize the results."
      ],
      "metadata": {
        "id": "Ppq3xp7fApK-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLxLvISd2Mm5"
      },
      "source": [
        "## 3. Cross-Validation+ (CV+)\n",
        "As seen during the lecture, practitionners are not always willing to sacrifice part of the training data for the calibration phase. Of course, this is not a problem for our toy case, where we can generate as many extra exmaple as we wish... but we will nevertheless implemnt\n",
        "the *Cross-Validation+* algorithm, which allows to use the whole training dataset for training, instead of splitting it into a proper training set and a calibration set. Of course, this comes at the cost of having to train multiple models.\n",
        "\n",
        "**Exercise.** Use the `KFold` class from `scikit-learn` to:\n",
        "1. Train 10 different linear regression models according to the CV+ algorithm.\n",
        "2. Compute the nonconformity scores on each of the folds.\n",
        "\n",
        "Store the models into a list of models called `models` and the scores in a numpy array called `scores`.\n",
        "\n",
        "**Warning.** Read carefully the documentaion of the `Kfold` function, we will be using it again further down to compute the prediction sets and the point predictions on the test sets, and we need to make sure that the data is split in the exact same folds both times!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: train and compute scores according to the CV+ algorithm"
      ],
      "metadata": {
        "id": "A7g57twnJrtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Use the CV+ algorithm to produce prediction intervals for nominal $\\alpha=0.1$. Compute point predictions on the test set by averaging accross the 10 different models."
      ],
      "metadata": {
        "id": "z417DAEReeRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Conformalize the model using the CV+ algorithm"
      ],
      "metadata": {
        "id": "qFpdlWNGJrq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Evaluate and plot the results."
      ],
      "metadata": {
        "id": "5FyqypAoT6x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Evaluate the results\n",
        "\n",
        "# TODO: plot the conformalized data"
      ],
      "metadata": {
        "id": "-Yi_4gUnT8th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Perform the Cross-Validation using PUNCC and compare the result, there is no tutorial for this algorithm, but you can check the following section in the documentation to help yourself:\n",
        "\n",
        "https://deel-ai.github.io/puncc/regression.html#deel.puncc.regression.CVPlus"
      ],
      "metadata": {
        "id": "lX7L7o8wJTTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that whereas the guarantee provided by the CV+ method is\n",
        "$$\\mathbb{P}(Y_{n+1}\\in \\hat{C}_\\alpha(X_{n+1})) \\geq 1-2\\alpha,$$\n",
        "however the achieved coverage rate is rather close to $1-\\alpha$.\n",
        "\n",
        "**Question.** How can you explain this?"
      ],
      "metadata": {
        "id": "1EGTxIRtdm3c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwvxw_Kr2Mm6"
      },
      "source": [
        "## 4. Conformal Quantile Regression\n",
        "We now turn to the problem of the *constant* prediction intervals. As we can see in the plots above, the coverage rate of $1-\\alpha$ is obtained by over-covering in the low-variance regions and over-covering in the high variance regions. We next consider the *Conformalized Quantile Regression (CQR)* algorithm, the purpose of which is to generate prediction sets that are more adapted to the heteroskedasticity of the data. CQR extends traditional quantile regression by incorporating conformal prediction techniques, allowing us to construct predictive intervals with state-of-the-art performance and guaranteed coverage (under data exchangeability).\n",
        "\n",
        "**Exercise.** Train lower and upper quantile models for a nominal eror rate $\\alpha$ of 0.1, using the `GradientBoostingRegressor` model with 10 estimators from the `sklearn.ensemble` module.\n",
        "\n",
        "**Careful!** The CQR algorithm is a kind of *split conformal* algorithm, so we neet to use the *fit* data split to train the models, and hold out the *calibration* data split for the calibration phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICHEON0i2Mm6"
      },
      "outputs": [],
      "source": [
        "# TODO: train the lower and upper quantile models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Complete the `plot_quantile_model` function below and use it to plot the test examples along with the lower and upper quantile model predictions."
      ],
      "metadata": {
        "id": "ZwGRrhO_hVyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_quantile_models(X, y, lower_quantile_model, upper_quantile_model):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sort_indices = np.argsort(X.flatten())\n",
        "    X_sorted = X[sort_indices]\n",
        "    y_sorted = y[sort_indices]\n",
        "    # TODO: plot the data\n",
        "    # TODO: plot the lower quantile predictions\n",
        "    # TODO: plot the upper quantile predictions\n",
        "    plt.xlabel(r\"$X$\")\n",
        "    plt.ylabel(r\"$Y$\")\n",
        "    plt.xticks(np.arange(22, step=2))\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_quantile_models(X_test, y_test, lower_quantile_model, upper_quantile_model)"
      ],
      "metadata": {
        "id": "l-8PdHTXhWWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Code the functions `cqr_score` and `cqr_set` so that we can use them along with the class `SplitConformal` above in order to implement the CQR algorithm. In order to keep a structure compatible with the `SplitConformal` class above, the `y_pred` input to the `cqr_score` and `cqr_set` functions is an array of length 2, where the first element contains the predictions of the lower quantile model and the second one the predictions of the upper quantile model.\n",
        "\n",
        "**Careful!** There is a typo in the lecture notes!"
      ],
      "metadata": {
        "id": "cHoDSFVRiONh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement the functions"
      ],
      "metadata": {
        "id": "044bBpvrixs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Conformalize the quantile regressors using the CQR algorithm and the calibration dataset."
      ],
      "metadata": {
        "id": "qWgRFmrFjbB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Conformalize according to the CQR algorithm"
      ],
      "metadata": {
        "id": "LgurDqrkjNQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Evaluate and visualize the results of the CQR conformalized model-"
      ],
      "metadata": {
        "id": "cTT_TJ3wkcQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: evaluate the results\n",
        "\n",
        "def plot_cqr_conformalized_data(X, y, y_pred_lower, y_pred_upper, y_lower, y_upper):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sort_indices = np.argsort(X.flatten())\n",
        "    X_sorted = X[sort_indices]\n",
        "    y_pred_lower_sorted = y_pred_lower[sort_indices]\n",
        "    y_pred_upper_sorted = y_pred_upper[sort_indices]\n",
        "    y_lower_sorted = y_lower[sort_indices]\n",
        "    y_upper_sorted = y_upper[sort_indices]\n",
        "\n",
        "    plt.scatter(X, y, alpha=0.6)\n",
        "    plt.plot(X_sorted, y_pred_lower_sorted, color=\"red\", linewidth=3)\n",
        "    plt.plot(X_sorted, y_pred_upper_sorted, color=\"red\", linewidth=3)\n",
        "    plt.fill_between(\n",
        "        X_sorted.flatten(),\n",
        "        y_lower_sorted,\n",
        "        y_upper_sorted,\n",
        "        alpha=0.3,\n",
        "        color=\"green\",\n",
        "    )\n",
        "    plt.xlabel(r\"$X$\")\n",
        "    plt.ylabel(r\"$Y$\")\n",
        "    plt.xticks(np.arange(22, step=2))\n",
        "    plt.tight_layout()\n",
        "    plt.grid(False)\n",
        "    plt.show()\n",
        "\n",
        "plot_cqr_conformalized_data(X_test, y_test, y_pred_test_lower, y_pred_test_upper, y_lower, y_upper)"
      ],
      "metadata": {
        "id": "DgiISGOWkglG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions.**\n",
        "- What do you observe?\n",
        "- Is this the expected behavior? Why?\n",
        "- How does the average interval width compare with the previous methods?\n",
        "\n",
        "**Exercise.** Implement the CQR algorithm in with PUNCC and compare the results with the ones obtained above."
      ],
      "metadata": {
        "id": "9U9FvTqbWSOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II: Conformal Classification"
      ],
      "metadata": {
        "id": "5dMp2GrgbUha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this section is to train a small neural network on the MNIST dataset and apply the Conformal Classification algorithms seen during the lecture."
      ],
      "metadata": {
        "id": "NtowNStI71Md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 1. Dataset\n",
        "**Exercise.**\n",
        "1. Import the MNIST dataset from keras.\n",
        "2. Split the training set into a proper training set, which we call the `fit` dataset, and a calibration set. Use the first 50_000 training points for the fit dataset and the remaining ones for the calibration dataset.\n",
        "3. Convert the labels to categorical, and save them into new arrays."
      ],
      "metadata": {
        "id": "694F_wwk4FN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Import and pre-process the data according to the instructions."
      ],
      "metadata": {
        "id": "fgshPvvT3BcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Model"
      ],
      "metadata": {
        "id": "uzozVxv54LqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Define a simple convolutional neural network having the following sequential architecture:\n",
        "- A convolution with kernel size 3 and 16 channels.\n",
        "- A ReLU activation.\n",
        "- A max pooling layer with kernel size 2.\n",
        "- A convolution with kernel size 3 and 32 channels.\n",
        "- A reLU activation.\n",
        "- A max pooling layer with kernel size 2.\n",
        "- A Fully connected layer with 10 neurons.\n",
        "- A softmax activation.\n",
        "\n",
        "**Warning!** Use tensorflow, it is easier to use with PUNCC later."
      ],
      "metadata": {
        "id": "7jPfgClX8vYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import random\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "random.set_seed(0)\n",
        "keras.utils.set_random_seed(0)\n",
        "\n",
        "# Classification model: convnet composed of two convolution/pooling layers\n",
        "# and a dense output layer\n",
        "nn_model = ...  # TODO: define the network"
      ],
      "metadata": {
        "id": "aL4YqMVs3G-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training\n",
        "\n",
        "**Exercise.** Train the model using the Adam optimizer, and the categorical cross-entropy loss. Plot the training and validation accuracy while training, with 10% of the fit data left out for the validation set.\n",
        "\n",
        "Train the model for 2 epochs with a batch size of 256."
      ],
      "metadata": {
        "id": "r2B3TJSj4PF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train the model for two epochs"
      ],
      "metadata": {
        "id": "b7hfuoCp4XHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Least Ambiguous Set-Valued Classifiers (LAC)\n",
        "We now implement the LAC algorithm seen during the lecture."
      ],
      "metadata": {
        "id": "iCA-LBOpbYAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Define a `lac_score` and a `lac_set` function in order to be used with the `SplitConformal` class above."
      ],
      "metadata": {
        "id": "oPEBOJdjn_Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: define both functions"
      ],
      "metadata": {
        "id": "baaKnFdufW7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Conformalize the classification model."
      ],
      "metadata": {
        "id": "DhS730qQpDOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_calib = nn_model.predict(X_calib)\n",
        "y_pred_test = nn_model.predict(X_test)"
      ],
      "metadata": {
        "id": "NtHFwbLjo7_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Conformalize the classifier using the calibration dataset and a nominal error rate of 0.1"
      ],
      "metadata": {
        "id": "aA-j5mnqoPnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Evaluate the results by computing the average coverage and average size of the prediction sets on the test set."
      ],
      "metadata": {
        "id": "lYVJxvgTqR5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_conformal_classifier(y_true, y_predset):\n",
        "    # TODO: implement the function to compute both metrics (coverage and set size)\n",
        "\n",
        "# TODO: Evaluate the conformalized model."
      ],
      "metadata": {
        "id": "ZkEw0X21qeBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise.** Plot a random image along with the prediction set."
      ],
      "metadata": {
        "id": "MuVue3wFq1uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = 18\n",
        "\n",
        "plt.imshow(X_test[sample].reshape((28,28)))\n",
        "_ = plt.title(f\"Point prediction: {np.argmax(y_pred[sample])} \\n Prediction set: {y_predset_test[sample]}\")\n",
        "_ = plt.xticks([])\n",
        "_ = plt.yticks([])"
      ],
      "metadata": {
        "id": "zbzQskSf7aVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions.**\n",
        "1. How come the average size of the prediction sets is smaller than 1 ?\n",
        "2. Some of the prediction sets are empty, they contain no labels. Why? Do you think that this is an Ok behavior?"
      ],
      "metadata": {
        "id": "zFxLJgQ97hNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Regularized Adaptive Prediction Sets\n",
        "**Exercise.** Follow the *Introduction Tutorial* in PUNCC, the section on *Conformal Classification* to implement the RAPS method as in the tutorial. Compare the results thus obtained with those obtained from the LAC method."
      ],
      "metadata": {
        "id": "t61p2NOE7Cee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To go further\n",
        "\n",
        "1. Code the APS and RAPS algorithms from scratch without using PUNCC.\n",
        "2. Check out the MAPIE library, which is a nice alternative to PUNCC.\n",
        "3. Check out the algorithms for Conformal Anomaly Detection and Conformal Object Detection in PUNCC."
      ],
      "metadata": {
        "id": "rWt0nXeK-dli"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cDZKVqre-zb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}