{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcPuszv4pumE"
      },
      "source": [
        "# OOD Detection\n",
        "The purpose of this lab project is to enhance our understanding of OOD detection. After accomplishing the lab project, you should be able to:\n",
        "- Code different OOD score functions and use them for OOD detection.\n",
        "- Perform benchmarking experiments involving different OOD score functions and different metrics.\n",
        "- Visualize OOD detection results and check for common mistakes in OOD detection experiments.\n",
        "\n",
        "As usual, we start by importing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFF9ViN7wTt4",
        "outputId": "4d1ef104-4b44-46c0-d4a4-67e15b6e897f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9ME3YsEuwkt"
      },
      "source": [
        "## 1. Data\n",
        "The ultimate purpose of this notebook is to perform a benchmarking experiment in order to compare multiple OOD scores and OOD detection algorithms. To that end, we will use three different data sets:\n",
        "1. The **Cifar-10 train** dataset in order to train a simple convolutional neural network for the task of image classification.\n",
        "2. The **Cifar-10 test** set as the *in-distribution* dataset (i.e. the dataset of normal examples), for evaluating the different OOD scores.\n",
        "3. (A subset of) The **SVHN test** set as the *out-of-distribution* dataset (i.e. the dataset of anomalous examples), for evaluating the different OOD scores.\n",
        "\n",
        "**Question.** Would it be fair to use the *Cifar-10 train* set along with the SVHN test set in order to evaluate the OOD detection scores and algorithms?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ2PXiekpT_N",
        "outputId": "5db842ae-f9a2-4b4e-e392-7e19ea60f2e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Using downloaded and verified file: ./data/test_32x32.mat\n"
          ]
        }
      ],
      "source": [
        "# Data loading and preprocessing\n",
        "batch_size = 128\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "cifar_train = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
        "cifar_test = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n",
        "svhn_test = datasets.SVHN(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "# Extract 10_000 random images from the svhn_test set\n",
        "svhn_test, _ = torch.utils.data.random_split(svhn_test, [10_000, len(svhn_test) - 10_000])\n",
        "\n",
        "train_loader = DataLoader(cifar_train, batch_size=batch_size, shuffle=True)\n",
        "cifar_test_loader = DataLoader(cifar_test, batch_size=batch_size, shuffle=False)\n",
        "svhn_test_loader = DataLoader(svhn_test, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VID62MuAlh_3",
        "outputId": "a08aaa4b-4bb4-40df-b45b-963fbc0501ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training samples: 50000\n",
            "Number of test samples: 10000\n",
            "Number of SVHN test samples: 10000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of training samples: {len(cifar_train)}\")\n",
        "print(f\"Number of test samples: {len(cifar_test)}\")\n",
        "print(f\"Number of SVHN test samples: {len(svhn_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haXIuOIWyhRd"
      },
      "source": [
        "## 2. CNN Classifier\n",
        "We will first train a CNN Classifier on the Cifar-10 training data, for the task of classifying the Cifar-10 images.\n",
        "\n",
        "The architecture of the CNN should be:\n",
        "- A convolutional layer with 32 filters, kernel size 3, stride 1 and padding 1.\n",
        "- A ReLU activation\n",
        "- A max pooling layer with kernel size 2.\n",
        "- A convolutional layer with 64 filters, kernel size 3, stride 1 and padding 1.\n",
        "- A ReLU activation\n",
        "- A max pooling layer with kernel size 2.\n",
        "- A fully connected layer with 128 neurons.\n",
        "- A ReLU activation (the activations after this layer will be called the **features of the penultimate layer**).\n",
        "- A fully connected layer with 10 neurons.\n",
        "\n",
        "This CNN will output the logit values.\n",
        "\n",
        "**Exercise** Define a CNN having the above architecture by implementing the `__init__` and `forward` methods below. Bare in mind that some of the OOD scores we will define require access to the features of the penultimate layer.\n",
        "- Add a `return_features` argument to the `forward` method, defaulting to `False`. If `return_features` is set to `True`, the `forward` method should return the features of the penultimate layer instead of the logit values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCxJ73k9iS4c"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        # TODO: define the necessary layers\n",
        "\n",
        "    def forward(self, x, return_features=False):\n",
        "        # TODO: apply the different layers to x in the correct order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/classifier.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWAWS1hvyq0J"
      },
      "source": [
        "## 3. Training\n",
        "We will train the above CNN on the Cifar-10 training set.\n",
        "\n",
        "**Exercise.** Train the CNN:\n",
        "- For 5 epochs\n",
        "- Using a learning rate of 0.001\n",
        "- Choose an appropriate loss function\n",
        "- Using the Adam optimizer\n",
        "- Print the mean loss of the epoch at the end of each epoch.\n",
        "- *Optional.* You can choose to monitor the training by printing the train/test accuracy too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYfG_bMxyoZu"
      },
      "outputs": [],
      "source": [
        "model = SimpleCNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnaQfKb6zKbt"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "# TODO: Set the number of epochs and the learning rate\n",
        "\n",
        "# Loss and optimizer\n",
        "# TODO: Set the loss function and the optimizer\n",
        "\n",
        "# Training loop\n",
        "def train_model():\n",
        "    # TODO: Implement the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/traininig.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOu4spKZzNal",
        "outputId": "a5dc16cb-66a0-4620-cae8-86c32eaede26"
      },
      "outputs": [],
      "source": [
        "train_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH0qDQjyPg0Q"
      },
      "source": [
        "**Exercise.** Print the test loss and the test accuray after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmyLFJJy0KS9"
      },
      "outputs": [],
      "source": [
        "# TODO: print test loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/classifier_eval.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxuio_z6hUv9"
      },
      "source": [
        "## 4. OOD Metrics\n",
        "The objective of this section is to define the different OOD metrics studied during the lectures. Recall that we have seen two kinf of metrics:\n",
        "1. Fixed-threshold metrics.\n",
        "2. Threshold-independent metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THtpVgN80wMx"
      },
      "source": [
        "### 4.1. Fixed-threshold metrics\n",
        "We will start to define the metrics for OOD detectors with a fixed threshold. The inputs to all of our metrics below will be:\n",
        "- The `scores_negatives` nupy array: an array containing the scores for the ground truth negative images (i.e. the Cifar-10 test images).\n",
        "- The `scores_positives` numpy array: an array containing the scores for the ground truth positive images (i.e. the SVHN test images).\n",
        "- The `threshold` floating point number. The threshold value $\\tau$ such that our OOD detector classifies examples according to their score as follwos:\n",
        "$$\\begin{cases}\n",
        "s \\leq \\tau\\quad &⇒\\quad \\text{negative}\\\\\n",
        "s > \\tau\\quad &⇒\\quad \\text{positive}\n",
        "\\end{cases}$$\n",
        "- Any other parameters necessary for the metric in question.\n",
        "\n",
        "**Exercise.** Define the functions below:\n",
        "1. A `confusion_matrix` function that outputs the number of *false positives*, *true positives*, *true negatives* and *false negatives*.\n",
        "2. A `tpr_fpr` function that outputs the  *true positive rate* and *false positive rate*.\n",
        "3. An `accuracy` function that outputs the accuracy.\n",
        "4. A `precission_recall` function that outputs the *precision* and the *recall*.\n",
        "5. An `f_beta` function that takes an additional input argument `beta` and returns the corresponding $F_\\beta$ score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK0fPuID0mv2"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix(scores_negatives, scores_positives, threshold):\n",
        "    # TODO: Compute and return the confusion matrix\n",
        "\n",
        "def tpr_fpr(scores_negatives, scores_positives, threshold):\n",
        "    # TODO: Compute and return the tpr and fpr\n",
        "\n",
        "def accuracy(scores_negatives, scores_positives, threshold):\n",
        "    # TODO: Compute and return the accuracy\n",
        "\n",
        "def precision_recall(scores_negatives, scores_positives, threshold):\n",
        "    # TODO: Compute and return the precission and recall\n",
        "\n",
        "def f_beta(scores_negatives, scores_positives, threshold, beta):\n",
        "    # TODO: Compute and return the f_beta score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/ftmetrics.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR3-4eu_SX6z"
      },
      "source": [
        "### 4.2. Threshold-independent metrics\n",
        "**Exercise.** Define the function `roc_auc` that:\n",
        "- Takes as input the `scores_negatives` and `scores_positives` numpy arrays.\n",
        "- Plots the *ROC curve*.\n",
        "- Returns the value of the *AUROC* as the area under the *ROC curve*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCp31Q4K-3cA"
      },
      "outputs": [],
      "source": [
        "def roc_auc(scores_negatives, scores_positives):\n",
        "    # TODO: Combine scores and create labels\n",
        "    scores = np.concatenate((scores_negatives, scores_positives))\n",
        "    labels = ... # TODO Give the label 0 to negative data and the label 1 to positive data\n",
        "\n",
        "    # Sort scores and labels\n",
        "    sorted_indices = np.argsort(scores)\n",
        "    scores = scores[sorted_indices]\n",
        "    labels = labels[sorted_indices]\n",
        "\n",
        "    # Initialize TPR and FPR\n",
        "    tpr = []\n",
        "    fpr = []\n",
        "    n_pos = np.sum(labels)\n",
        "    n_neg = len(labels) - n_pos\n",
        "\n",
        "    tp = n_pos\n",
        "    fp = n_neg\n",
        "\n",
        "    # TODO: loop through all possible thresholds (i.e. all possible scores)\n",
        "    # and update the number of true positives and false positives for eac threshold.\n",
        "    # Compute the respective tpr and fpr and append them to the tpr and fpr lists.\n",
        "\n",
        "    # Convert the tpr and fpr lists to numpy arrays\n",
        "    tpr = np.array(tpr)\n",
        "    fpr = np.array(fpr)\n",
        "\n",
        "    # Compute AUROC (Area Under the Curve)\n",
        "    auroc = # TODO: Compute the AUC using the np.trapz function\n",
        "\n",
        "    # TODO: Plot ROC curve\n",
        "\n",
        "    return auroc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/roc_auc.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzxzEdZmUMN5"
      },
      "source": [
        "\n",
        "## 5. OOD Scores\n",
        "In this section, we will implement the different OOD scores seen during the lecture. Recall that we can split the different OOD scores into two score families:\n",
        "1. Logit-based scores.\n",
        "2. Feature-based scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMc-JNRshQX8"
      },
      "source": [
        "### 5.1. Logit-based scores\n",
        "Logit-based scores are simpler to implement than feature-based scores. We will implement each of the logit-based scores as a function that takes as inputs the `logits` array of logits of the different test points,\n",
        "and returns the array of test point scores.\n",
        "\n",
        "**Exercise.** Complete the functions below with the formulas seen during the lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stSGgea-2z1M"
      },
      "outputs": [],
      "source": [
        "# MLS Score\n",
        "def mls(logits):\n",
        "    # TODO: Compute and return the MLS score\n",
        "\n",
        "# MSP Score\n",
        "def msp(logits):\n",
        "    # TODO: Compute and return the MSP score\n",
        "\n",
        "# Energy Score\n",
        "def energy(logits, temp=1):\n",
        "    # TODO: Compute and return the Energy score\n",
        "\n",
        "# Entropy Score\n",
        "def entropy(logits):\n",
        "    # TODO: Compute and return the Entropy score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/logit_scores.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JopT8Bl5EI2w"
      },
      "source": [
        "### 5.2. DKNN\n",
        "In this section we define a class `DKNN` to compute the Deep $K$-nearest neighbor score. This score is more involved than the previous ones for two main reasons:\n",
        "- It employs the activations of the penultimate layer of the CNN rather than the logit or softmax values.\n",
        "- It requires a fitting dataset in order to compute distances of the test images with respect to the images in the fitting dataset. We will be using the Cifar-10 training set as fitting dataset.\n",
        "\n",
        "*Exercise.* Complete the following methods in the class `DKNN` below:\n",
        "1. The `_l2_normalization` method that normalizes a batch of feature vectors by dividing each feature vector by its $\\ell_2$ norm.\n",
        "2. The `compute_scores` function that computes the distance from each of the test points to its $k$-th nearest neighbor in the fit dataset. The distances are computed between the normalized feature representations. The test points are processed in batches to avoid memory issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFRquLyzEHPG"
      },
      "outputs": [],
      "source": [
        "class DKNN:\n",
        "    def __init__(self, k=50, batch_size=256):\n",
        "        self.k = k\n",
        "        self.batch_size = batch_size\n",
        "        self.fit_features = None\n",
        "\n",
        "    def _l2_normalization(self, feat):\n",
        "        norms = ... # TODO: Compute the norm of each feature vector, and add a small constant to it to avoid dividing by zero\n",
        "        return feat / norms\n",
        "\n",
        "    def fit(self, fit_dataset):\n",
        "        self.fit_features = # TODO: Apply the l2 normalization to the fit dataset.\n",
        "\n",
        "    def compute_scores(self, test_features):\n",
        "        test_features = ... # TODO: Apply the l2 normalization to the test dataset.\n",
        "        scores = []\n",
        "\n",
        "        # Process test features in batches\n",
        "        for i in range(0, test_features.size(0), self.batch_size):\n",
        "            batch = test_features[i:i + self.batch_size]\n",
        "            # Compute pairwise distances for the batch\n",
        "            distances = torch.cdist(batch, self.fit_features, p=2)  # (batch_size, num_fit_samples)\n",
        "            # TODO: Sort distances and extract the k-th nearest\n",
        "            # Append the results to the list of scores.\n",
        "\n",
        "\n",
        "        # Concatenate scores from all batches\n",
        "        return torch.cat(scores, dim=0).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/dknn.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC9JTzaUHQQR"
      },
      "source": [
        "### 5.3. Mahalanobis\n",
        "In this section we define a class `Mahalanobis` to compute the Mahalanobis score. This class is similar to the `DKNN` for the same reasons as before:\n",
        "- It employs the activations of the penultimate layer of the CNN rather than the logit or softmax values.\n",
        "- It requires a fitting dataset in order to compute distances of the test images with respect to the images in the fitting dataset. We will be using the Cifar-10 training set as fitting dataset.\n",
        "\n",
        "*Exercise.* Complete the following methods in the class `Mahalanobis` below:\n",
        "1. The `fit` method that fits per-class mean vectors and a common covariance matrix to the fitting dataset.\n",
        "2. The `_mahalanobis_distance` method that computes the Mahalanobis distance of a given vector with respect to the gaussian law parametrized by its mean vector and covariance matrix.\n",
        "3. The `compute_scores` function that uses the two previous methods to compute the Mahalanobis score of all test points by taking the maximum of Mahalanobis distances over the set of different classes/labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_raBxGqGynP"
      },
      "outputs": [],
      "source": [
        "class Mahalanobis():\n",
        "    def __init__(self):\n",
        "        self.mus = None\n",
        "        self.inv_cov = None\n",
        "        self.labels = None\n",
        "\n",
        "    def fit(self, features, labels):\n",
        "        self.labels = # TODO: extract the set of unique labels\n",
        "        self.mus = {}\n",
        "        covs = {}\n",
        "        for label in self.labels:\n",
        "            # TODO: fit the mean vector corresponding to the label\n",
        "            # and the RESCALED covariance matrix\n",
        "\n",
        "        cov = # TODO: Compute the common covariance matrix for all labels\n",
        "        self.inv_cov = # TODO: Compute the (pseudo-)inverse of the covariance matrix\n",
        "\n",
        "    def _mahalanobis_distance(self, x, mu, inv_cov):\n",
        "        # TODO: Compute and return the Mahalanobis distance for the given mean and inverse covariance\n",
        "\n",
        "    def compute_scores(self, test_features):\n",
        "        scores = []\n",
        "        for test_feature in test_features:\n",
        "            distances = # TODO: Compute the vectore of per-label Mahalanobis distances\n",
        "            # TODO: Compute the mahalanobis score of the current test example and append it to the list of scores.\n",
        "        return torch.stack(scores).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/mahalanobis.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOTwj3XfGk8K"
      },
      "source": [
        "## 6. Score Comparison\n",
        "The objective of this section is to compare the different OOD scores that we have just defined. Note that in order to use the *threshold-dependent metrics*, we need to pick a threshold for each of the scores.\n",
        "\n",
        "Picking the same threshold for all scores *is not* a proper way to compare the different scores, since thy are scaled differently. A common way to perform a more \"fair\" comparison is to do the following:\n",
        "1. Fix a target TPR, e.g. 0.95.\n",
        "2. Compute the threshold $\\tau$ such that the TPR on the SVHN test dataset is equal to the target TPR 0.95.\n",
        "3. Compute the remaining fixed-threshold metrics for such $\\tau$.\n",
        "\n",
        "**Exercise.** Define the function `compute_threshold` that:\n",
        "- Takes as inputs `scores`, a numpy array of scores and a `target_tpr`, a value between 0 and 1 defaulting to 0.95.\n",
        "- Assuming that the array of `scores` contains the scores of the positive examples, the function computes and returns the value of the threshold $\\tau$ that achieves the desired `target_tpr`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMpdncvKGfaJ"
      },
      "outputs": [],
      "source": [
        "def compute_threshold(scores, target_tpr=0.95):\n",
        "    # TODO: Compute and return the desired threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/compute_threshold.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orF-EhfcVxvT"
      },
      "source": [
        "In order to compare the different OOD scores that we have defined, we set the variable `target_tpr` equal to 0.9 and we initialise an empty dictionary to store the different metrics for the different OOD scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG3eR20wGppC"
      },
      "outputs": [],
      "source": [
        "target_tpr = 0.9\n",
        "metrics_dict = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t3zX3pYW5bM"
      },
      "source": [
        "### 6.1. Metrics for logit-based scores\n",
        "\n",
        "**Exercise.** Next we compute the different evaluation metrics for each of the scores above, starting with the *logit-based scores*:\n",
        "1. Extract the logits of the Cifar-10 test set and the SVHN test set.\n",
        "2. For each of the *MLS*, *MSP*, *Energy (T=1)* and *Entropy* OOD score functions:\n",
        "  - Compute the scores on the Cifar-10 test set and the SVHN test set.\n",
        "  - Plot the histogram of the scores and check that the negative samples have, on average, lower scores than the positive samples.\n",
        "  - Use the `roc_auc` function to plot the ROC curves and compute the AUROCs.\n",
        "  - Compute the trhreshold that achieves 0.1 FPR and compute the fixed-threshold metrics associated to it: accuracy, TPR, Precision, Recall and $F_1$.\n",
        "  - Store all the metrics in the `metrics_dict` dictionary for future comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Updp2_mpfb7j"
      },
      "outputs": [],
      "source": [
        "# Compute logits directly from the dataset\n",
        "def compute_logits(dataset, model, device):\n",
        "    # TODO: Compute and return the logits of the elements in the dataset as a torch tensor.\n",
        "\n",
        "# Apply the function to CIFAR-10 and SVHN datasets\n",
        "test_logits_negatives = compute_logits(cifar_test, model, device)\n",
        "test_logits_positives = compute_logits(svhn_test, model, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/compute_logits.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9SW-wTyW8AF"
      },
      "outputs": [],
      "source": [
        "scoring_functions = {\n",
        "    'MLS': mls,\n",
        "    'MSP': msp,\n",
        "    'Energy': energy,\n",
        "    'Entropy': entropy\n",
        "}\n",
        "\n",
        "for method, scoring_function in scoring_functions.items():\n",
        "\n",
        "    # TODO: Compute scores\n",
        "    scores_negatives = ...\n",
        "    scores_positives = ...\n",
        "\n",
        "    # TODO: Plot histogram of scores\n",
        "\n",
        "    # Initialize empty dict for metrics\n",
        "    metrics_dict[method] = {}\n",
        "\n",
        "    # TODO: Plot ROC curve and compute AUROC\n",
        "    auroc = ...\n",
        "    metrics_dict[method]['auroc'] = auroc\n",
        "\n",
        "    # TODO: Compute threshold for the given target_tpr\n",
        "    threshold = ...\n",
        "\n",
        "    # TODO: Compute and store remaining metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load logits_comparison.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXnQfea1XvGZ"
      },
      "source": [
        "### 6.2. Metrics for feature-based scores\n",
        "\n",
        "**Exercise.** Extract the representations in the feature space given by the penultimate layer of the CNN of the three datasets: Cifar-10 training dataset, Cifar-10 test set and SVHN test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkD-GUAGxOsU"
      },
      "outputs": [],
      "source": [
        "# Compute features directly from the dataset\n",
        "def compute_features(dataset, model, device):\n",
        "    # TODO: Compute and return the feature representations of the elements in the dataset as a torch tensor.\n",
        "\n",
        "\n",
        "# TODO: Extract the features of the CIFAR-10 train, test, and SVHN test datasets\n",
        "fit_features = ...\n",
        "test_features_negatives = ...\n",
        "test_features_positives = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/compute_features.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VX2JvpuYEnp"
      },
      "source": [
        "**Exercise.**\n",
        "1. Compute the *DKNN scores* for the Cifar-10 test dataset and the SVHN test datsets using the 5-th nearest neighbor.\n",
        "2. Plot the histogram of the scores and check that the negative samples have, on average, lower scores than the positive samples.\n",
        "2. Use the `roc_auc` function to plot the ROC curve and compute the AUROC.\n",
        "3. Compute the trhreshold that achieves 0.1 FPR and compute the fixed-threshold metrics associated to it: accuracy, TPR, Precision, Recall and $F_1$.\n",
        "4. Store all the metrics in the `metrics_dict` dictionary for future comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJkeW3oy7-LV"
      },
      "outputs": [],
      "source": [
        "metrics_dict['DKNN'] = {}\n",
        "\n",
        "# TODO: initialize and fit the DKNN model\n",
        "\n",
        "# TODO: Compute the scores of the negative and positive data from their feature representations\n",
        "\n",
        "# TODO: Plot the histogram of the scores\n",
        "\n",
        "# TODO: Plot ROC curve and compute AUROC\n",
        "auroc = ...\n",
        "metrics_dict['DKNN']['auroc'] = auroc\n",
        "\n",
        "# TODO: Compute threshold for the given target_tpr\n",
        "threshold = ...\n",
        "\n",
        "# TODO: Compute and store remaining metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/dknn_metrics.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1dyipW0YSLU"
      },
      "source": [
        "**Exercise.**\n",
        "1. Compute the *Mahalanobis scores* for the Cifar-10 test dataset and the SVHN test datsets using the 5-th nearest neighbor.\n",
        "2. Plot the histogram of the scores and check that the negative samples have, on average, lower scores than the positive samples.\n",
        "2. Use the `roc_auc` function to plot the ROC curve and compute the AUROC.\n",
        "3. Compute the trhreshold that achieves 0.1 FPR and compute the fixed-threshold metrics associated to it: accuracy, TPR, Precision, Recall and $F_1$.\n",
        "4. Store all the metrics in the `metrics_dict` dictionary for future comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGtk3cMq-Lnn"
      },
      "outputs": [],
      "source": [
        "metrics_dict['Mahalanobis'] = {}\n",
        "\n",
        "# TODO: initialize and fit the Mahalanobis model\n",
        "\n",
        "# TODO: Compute the scores of the negative and positive data from their feature representations\n",
        "\n",
        "# TODO: Plot the histogram of the scores\n",
        "\n",
        "# TODO: Plot ROC curve and compute AUROC\n",
        "auroc = ...\n",
        "metrics_dict['Mahalanobis']['auroc'] = auroc\n",
        "\n",
        "# TODO: Compute threshold for the given target_tpr\n",
        "threshold = ...\n",
        "\n",
        "# TODO: Compute and store remaining metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load solutions/mahalanobis_metrics.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1BB9H-HBHyn"
      },
      "source": [
        "## Results Table\n",
        "**Exercise.** Plot the results stored in the dictionary `metrics_dict` by highlighting the method that achieves the best value for each of the different metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PknDmPmxBsJ7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# TODO: Display a table with the best results highlited.\n",
        "# Careful! The best result is not always the maximum value!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load results_table.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcmhGIHABXlT"
      },
      "source": [
        "**Bonus Exercises.** If you still have time, you can try and do the following:\n",
        "1. Play with different temperature parameters in the *Energy* score to see how they affect the different metrics.\n",
        "2. Play with different $k$ parameters in the *DKNN* algorithm to see how they affect the different metrics.\n",
        "3. Write docstirngs for the above function (In the future, you will be greatful to your current self if you find yourself checking out this notebook and the docstrings are there).\n",
        "4. Download a better model (e.g. a pre-trained VGG model fine-tuned on Cifar-10) and check out if you get better results with it.\n",
        "5. Check out the OODEEL library where a benchmark like the one we have just carried-out is much easier to perform ;)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
